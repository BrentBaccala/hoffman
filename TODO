
- more clever encoding

En passant is what makes lasker1901 so ridiculously big with
combinadic3 encoding.

Allow kings and side-to-move-flag to be positioned within index. Right
now, side to move flag is always the low bit, and kings always come
next.

Allow an 'offset' field to adjust the range of the fields, i.e, allow
DTM field to range from -10 to 5 and still fit into 4 bits.


- fast movecnt generation with perfect hashing

- a testsuite

Initially, I used the Nalimov tablebases to check Hoffman tablebases
for accuracy, but as Hoffman has become more sophisticated, this
solution is less and less acceptable.  Hoffman needs a capability to
check a tablebase for integrity - Are all of its XML statistics
correct?  Are all of the DTM values in its entries internally
consistent?  Are they consistent with its futurebases?  This new
feature should be used within a dejagnu framework to systematically
verify program operation within as few runs as feasible.


- verify futurebase format compatibility

There are actually hidden bugs here, because we don't ensure that
futurebase formats are consistent with the one we are trying to
generate.  For example, if we're trying to generate a 'flag' format,
we have to make sure that any flags in the futurebases are of the
correct "sense"... and that sense is inverted if the futurebase is
inverted!  Also, as a sanity check, I think we shouldn't allow 'dtm'
metrics to be created from 'flag' or 'basic' futurebases... because
they're not really distance-to-mates!  Instead, let's go ahead and add
a 'dtc' (distance-to-conversion) metric or maybe 'dtf'
(distance-to-futurebase) would be a better name, to handle back
propagation with distances from futurebases that have none.


- general speed improvements

The program can operate in two modes (in-memory or proptables), and
the two behave radically differently performance-wise.

For in-memory operation, the most immediate place that can probably
use attention is the initialization code.  Bitboard representation can
probably be used more effectively; see some of the papers on Bob
Hyatt's website for ideas.  Intel processors have bit scan
instructions (BSF/BSR) that search a register for the first or last 1
bit, we probably want to make sure we use these when counting
movements (logical AND to find what pieces we can hit, then bit scan
to find which one we hit first).  Perfect hashing (magic bitboards)
look even better.  Also, when we advance the loop by a single index,
we're typically moving a single piece (or none at all, just flipping
the side-to-move flag), so this can be optimized.  This would also be
useful during futurebase backprop.  Might want to hand-code critical
sections of this loop in assembly, and there should be a big
difference between a 32-bit and a 64-bit architecture (since bitboards
have 64 bits).  I'll also note that some things (PTM_vector) only need
to be computed during initialization.

The tablebase operation, the main thing right now is to insure that
the program really becomes disk-bound.  Also want to check to see if
keeping the input and output files (or some similar split) on seperate
disks/controllers can be exploited.


- use an FPGA

Use of a Field-Programmable Gate Array (FPGA) is perhaps the single
best performance enhancement possible with Hoffman.  Using one of the
knjn.com boards, for example - the PCI-attached Dragon or the
USB-attached Xylo - we could offload the compute-intensive parts to a
pipelined coprocessor.  Initialization (where we just need to spit out
a stream of bytes) and backpropagation (where we need to back an index
out) could both be pipelined effectively.  Moving up to something like
the Digilent Virtex-5 development board could make six piece
tablebases practical.


- improve compressibility

Hoffman tablebases are significantly larger than Nalimovs.  Probably
the biggest help would be to clump like positions together.  Right
now, the index LSB is the side-to-move flag, we'd really like it to be
elsewhere (like the MSB) when one side is dominant and is going to
have the bulk of the mates.  A couple places in the code depend on the
side-to-move flag being LSB; they need to be found, investigated, and
made more flexible.  Then we can add a <side-to-move-flag/> element to
the XML, that gets put into the pieces list at the position that we'd
like the flag to be.

Also, we could use a modified gzip library that lets us flag certain
bytes as 'irrelevant', which instructs the library to fill these bytes
using the shortest possible encoding.  There's always going to be
illegal positions, no matter how cleaver our encoding scheme.

Another idea would be to store DTM information for only positive DTMs.
The negative DTMs would have to be interpolated at decode time, and
their positions in the tablebase file would be filled with a common
(and thus easily compressed) value.  Nalimov uses a variant of this
idea by splitting tablebases into two separate files depending on the
side-to-move.  This idea could be expanded by only storing DTM
information modulo some settable number.


- ability to reformat tablebases

Only once we're all done making a tablebase do we know stuff like the
max DTM.  I've got several that I build with 16-bit dtm that I now
know only need 8-bit.  It would be nice to reformat them.  It'd also
be nice to play around with different piece orderings (see item above)
to see which one gives the best compression without having to
re-generate the entire tablebase.  Note that attempting to reformat a
tablebase with finite field inversion into one without (it would be
nice) presents the same problem that inversion was designed to avoid
in the first place.


- improved checkpointing ability

Right now, we have a limited checkpointing ability if we're running
with proptables (the most time consuming case anyway) - the program
will write a checkpoint.xml file at the start of each pass which can
be used to restart the last pass in the event of something like a
system crash.  This could be really be cleaned up and improved upon.
With separate input and output entries files, I don't see any reason
it can't restart right where it left off, though we do have to be
careful to back up and reconstruct the last proptable that was in
memory at the time of the crash.  Maybe write the checkpoint file with
each proptable write, record the size of the proptables for
verification, and keep a backup copy of the checkpoint file in case we
crash during the checkpoint.  It will have to change significantly,
however, if we switch to using a single entries file in read/write
mode.  I've thought of putting an extra bit into the entries format
that starts off as 0, gets flipped to 1 as we make one pass through
the file, then gets flipped back to 0 on the next pass.  We would also
like to deal with the case where we get well into a calculation and
the dtm field overflows - just checkpoint, terminate, and use our
(unimplemented) reformating ability.


- fix libcurl handling of FTP put transactions and use it exclusively

I had a lot of problems using libcurl for FTP writes, so much so that
I switched to ftplib for FTP URLs.  Since libcurl advertises FTP
support, somebody could profitably investigate the problems and fix
them.


- add 'dtc' and 'dtr' formats

The distance to mate (dtm) metric that we use is the simplest but
maybe not the best.  Distance to conversion (dtc) tracks (basically)
how far until the next futurebase, and distance to rule (dtr)
incorporates the 50-move rule into the calculation, but is tricky to
implement - Guy Haworth has written some papers about this.


- be more consistent about current tablebase being a global var


- check indexing logic with an automated theorem prover

I've had a lot of bugs in the indexing logic, especially since there
are two different routines for each index type that need to be kept in
sync.  If we optimize the case where the index number is advanced by
one (see above), then we'll have a third routine to keep in sync.  Can
we use an automated theorem prover like Prover9 to ensure that this
code is right?


- stalemate pruning

This idea is to speed up processing by allowing promotion futurebases
with only queens to substitute for promotions into rooks and bishops,
since the only logical reason to underpromote is to avoid a stalemate.
To this end, a futurebase with an extra queen and stalemates pruned
would be treated as handling all three promotions (queen, rook, and
bishop).  To implement this correctly, you'd have to make sure that
stalemates were pruned not only in the current tablebase, but in all
(and I mean ALL) futurebases.  So this would mean adding an element to
tablebase-statistics, <all-stalemates-pruned color="white|black"/>,
that would flag when all futurebases depending on it had their
stalemates pruned as well.  This element would back propagate through
the futurebases and be dropped at any point where stalemate pruning
was turned off (unless there were no stalemates, like kk.htb).  In any
event, I've got the code to group all three promotions together in
RCS, but I've ripped it out of the main line since this extra logic
has never been implemented.


- ability to back prop from a Nalimov tablebase

Nalimov tablebases have been precomputed for 6-piece endgames;
currently, we've only got 5-piece endgames for Hoffman, so to leverage
the work that's already been done and to make Hoffman fully as
powerful as Freezer, we'd like to back prop directly from a Nalimov
tablebase.  With a good description of the Nalimov file format (which
I don't have), this shouldn't been difficult.  Also consider that
Hoffman tablebases are (currently) much bigger than Nalimov
tablebases, and fully 1TB of storage is required for a full set of
6-piece Nalimovs.


- restrict range of indices we look at during a backprop

If we're backproping, say, from kqk into kpk, we need only consider
those positions where the queen is on the eighth rank.  If the queen
is encoded into the MSBs of the index, then we should be able to
figure out at what futurebase index we need to start processing, and
fast forward to that point.  Symmetry might disrupt this, but the
general idea might still work, especially if it was done in a general
enough fashion, maybe by constructing a bitboard of legal board
positions for each piece at the beginning of a backprop (of any kind)
and fast forwarding over chunks of the futurebase that have been
identified as irrelevant.  The index-ordering=reverse attribute was
created to facilitate this, but it's never been implemented.


- add a repeat option for processing multiple XML files

If we're doing distributed batch processing, we'd probably like to
provide a URL to the program to get a generation control file (this
already works), then retry that URL when we're all done to get another
control file, and keep doing this.  Have to make sure the program is
pretty clean in its memory usage for this to work well.  Also should
provide some way to terminate the program at the end of the current
tablebase build.


- prune after some number of passes

If we're looking to trade some accuracy for speed, we can prune at
almost any point in the calculation by conceding every position that
hasn't been resolved.  So, for example, if we anticipate that allowing
black to queen will lose the game, except that we still want to
consider the possibility of immediately capturing the new queen
(Freezer can do this), we can run for one or two passes and then prune
everything else.  Note that in this example, we can already achieve
almost the same result by freezing the queen on the queening square
and conceding any move by the queen as a win to black.  We'd probably
run for several passes, though, checking lines where white can check
black around for a few moves before taking the queen.


- some kind of GUI

I like leaving hoffman as a standalone computation engine.  For one
thing, this makes it easier to compile on a minimal system where
you're not going to do anything except farm out tablebase
calculations.  So my current thought for a GUI would be either a
Perl/Tk script or a Java applet, and I think Java is much cleaner than
Perl.  Freezer seems to have a nice GUI for a tablebase generator; see
the demos on http://www.freezerchess.com/

Even the computation engine could use a GUI, though, especially on
Microsoft systems where it's the standard user interface.  A modal
startup sequence could begin with radio buttons to select either a
file chooser to pick a control file (or files) for processing, or a
URL to connect to a remote webserver and retrieve control files from
there.  An options panel could allow selecting the maximum amount of
RAM to use, along with the location of a temporary files directory and
the maximum amount of disk space to use.  Use of proptable mode would
then be automatic depending on how much RAM is available.  It should
be possible to change these settings while the program is running.
Once the analysis proper starts, you should be able to halt or pause
it, as well as iconize it while it continues to run.


- pawngen

Needs to be more sophisticated.  Needs to handle restricted pieces.
Needs to apply pruning statements selectively in different situations
(there's already some code in it to do this using Perl expressions).

Check for conflicting prune statements, so we don't have to wait until
we get into a hoffman run before we find out about them.

Add ability to restrict piece locations.  This should let us do the
fortress example with a single control file.

Add 'freeze' pruning type for promotions - promote and freeze new
piece on the promotion square.  This lets us queen a pawn and then let
it sit there on the queening square to ensure that the other side
can't either capture it immediately or queen right after us.


- improve probe mode's move parser

Check illegal moves more aggressively; have a 'back' command; maybe a
'board' command to print a diagram of the chess board.


- warn of incompatibilities between tb and entries formats


- random access to tablebase files

Would require a modified gzip format.  Nalimov used a variant of
Lempel-Ziv developed by Andrew Kadatch; I'm not sure how it works.
Maybe preprocess the file to develop a single, constant Huffman table
and store it at the beginning of the file along with a table of byte
offsets that can be binary searched to figure where in the file you
need to start decompressing.  Other people have looked at this; a
literature search seems to be in order.


- revisit all the XXXs

  - check for movecnt overflow when merging proptable entries
  - investigate proptable merge code in more detail
  - check stalemate handling in general
