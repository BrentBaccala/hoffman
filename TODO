
- more clever encoding

En passant is what makes lasker1901 so ridiculously big with
combinadic3 encoding.

Allow kings and side-to-move-flag to be positioned within index. Right
now, side to move flag is always the low bit, and kings always come
next.

Allow an 'offset' field to adjust the range of the fields, i.e, allow
DTM field to range from -10 to 5 and still fit into 4 bits.


- fast movecnt generation with perfect hashing

This might be more difficult because of the presence of futuremoves.
Counting moves isn't enough; we have to identify futuremoves as well.
Nevertheless, I think it can be done and should produce a big speedup.
Use a different hash function / magic number for each square a piece
can be on.  Then hash the board vector to move the relevent bits down
to the LSBs and lookup in a table to get movecnt along with flags
to indicate if has the piece moved to a restricted square.


- a testsuite

Initially, I used the Nalimov tablebases to check Hoffman tablebases
for accuracy, but as Hoffman has become more sophisticated, this
solution is less and less acceptable.  Hoffman needs a capability to
check a tablebase for integrity - Are all of its XML statistics
correct?  Are all of the DTM values in its entries internally
consistent?  Are they consistent with its futurebases?  This new
feature should be used within a dejagnu framework to systematically
verify program operation within as few runs as feasible.  Use a code
coverage tool.


- verify futurebase format compatibility

There are actually hidden bugs here, because we don't ensure that
futurebase formats are consistent with the one we are trying to
generate.  For example, if we're trying to generate a 'flag' format,
we have to make sure that any flags in the futurebases are of the
correct "sense"... and that sense is inverted if the futurebase is
inverted!  Also, as a sanity check, I think we shouldn't allow 'dtm'
metrics to be created from 'flag' or 'basic' futurebases... because
they're not really distance-to-mates!  Instead, let's go ahead and add
a 'dtc' (distance-to-conversion) metric or maybe 'dtf'
(distance-to-futurebase) would be a better name, to handle back
propagation with distances from futurebases that have none.


- general speed improvements

The program can operate in two modes (in-memory or proptables), and
the two behave radically differently performance-wise.

For in-memory operation, the most immediate place that can probably
use attention is the initialization code.  Bitboard representation can
probably be used more effectively; see some of the papers on Bob
Hyatt's website for ideas.  Intel processors have bit scan
instructions (BSF/BSR) that search a register for the first or last 1
bit, we probably want to make sure we use these when counting
movements (logical AND to find what pieces we can hit, then bit scan
to find which one we hit first).  Perfect hashing (magic bitboards)
look even better.  Also, when we advance the loop by a single index,
we're typically moving a single piece (or none at all, just flipping
the side-to-move flag), so this can be optimized.  This would also be
useful during futurebase backprop.  Might want to hand-code critical
sections of this loop in assembly, and there should be a big
difference between a 32-bit and a 64-bit architecture (since bitboards
have 64 bits).  I'll also note that some things (PTM_vector) only need
to be computed during initialization.

The tablebase operation, the main thing right now is to insure that
the program really becomes disk-bound.  Also want to check to see if
keeping the input and output files (or some similar split) on seperate
disks/controllers can be exploited.


- use an FPGA

Using one of the knjn.com boards, for example - the PCI-attached
Dragon or the USB-attached Xylo - we could offload the
compute-intensive parts to a pipelined coprocessor.  Initialization
(where we just need to spit out a stream of bytes) and backpropagation
(where we need to back an index out) could both be pipelined
effectively.


- improve compressibility of tablebases

Hoffman tablebases are significantly larger than Nalimovs.  Probably
the biggest help would be to clump like positions together.  Right
now, the index LSB is the side-to-move flag, we'd really like it to be
elsewhere (like the MSB) when one side is dominant and is going to
have the bulk of the mates.  A couple places in the code depend on the
side-to-move flag being LSB; they need to be found, investigated, and
made more flexible.  Then we can add a <side-to-move-flag/> element to
the XML, that gets put into the pieces list at the position that we'd
like the flag to be.

Also, we could use a modified gzip library that lets us flag certain
bytes as 'irrelevant', which instructs the library to fill these bytes
using the shortest possible encoding.  There's always going to be
illegal positions, no matter how cleaver our encoding scheme.

Another idea would be to store DTM information for only positive DTMs.
The negative DTMs would have to be interpolated at decode time, and
their positions in the tablebase file would be filled with a common
(and thus easily compressed) value.  Nalimov uses a variant of this
idea by splitting tablebases into two separate files depending on the
side-to-move.  This idea could be expanded by only storing DTM
information modulo some settable number.


- improve compressibility of entries/proptable files

Towards the end of a generation run, the program's time is dominated
by compressing the entries file.  By that point, we have a pretty good
idea what the entries file looks like, because we're only making a few
changes to it on each pass.  We could probably improve both speed and
compressibility by keeping a count of total entries of each type and
using a statistical encoding scheme based on what the counts were at
the start of each pass (they're not changing much).  Maybe start using
zlib for initial compression, then switch to a statistical scheme at
some point.

We could probably improve proptable compressibility by storing offsets
instead of individual proptable entries (remember, it's sorted).

Maybe play around with compressibility by adding XML controls to set
zlib compression/time tradeoff.


- ability to reformat tablebases

It'd be nice to play around with different piece orderings (see item
above) to see which one gives the best compression without having to
re-generate the entire tablebase.


- checkpointing ability

Right now, we have no checkpointing ability.  We used to have a
limited checkpointing ability with proptables (the most time consuming
case anyway) - the program would write a checkpoint.xml file at the
start of each pass which can be used to restart the last pass in the
event of something like a system crash.  This could be really be
cleaned up and improved upon.  I'm thinking about writing an XML
header on the entries file to enable it to be used to restart the
program.  We need to verify the file size, but if we know the formats,
then we know the (uncompressed) file sizes, so that shouldn't be too
hard.

Restarting in the middle of a pass requires using a partially
constructed entries file, which seems dicey.  Better, I think, would
be to back up and start by reruning the pass that generated the last
complete entries file, but with no input proptables, so it just
generates a new set of output proptables.

- Hadoop port

Apache Hadoop is currently (2012) the state of the art for cluster
computing.  To exploit Hadoop-type parallelism, we should allow for
partial entries files that contain only a sub-range of indices.  A
single program invocation applies a set of proptables to a partial
entries file and produces an output partial entries file, along with
output proptables segmented into sub-ranges.  Then the output
proptables get shuffled around between the compute nodes before the
next pass.


- fix libcurl handling of FTP put transactions and use it exclusively

I had a lot of problems using libcurl for FTP writes, so much so that
I switched to ftplib for FTP URLs.  Since libcurl advertises FTP
support, somebody could profitably investigate the problems and fix
them.


- add 'dtc' and 'dtr' formats

The distance to mate (dtm) metric that we use is the simplest but
maybe not the best.  Distance to conversion (dtc) tracks (basically)
how far until the next futurebase, and distance to rule (dtr)
incorporates the 50-move rule into the calculation, but is tricky to
implement - Guy Haworth has written some papers about this.


- be more consistent about current tablebase being a global var


- check indexing logic with an automated theorem prover

I've had a lot of bugs in the indexing logic, especially since there
are two different routines for each index type that need to be kept in
sync.  If we optimize the case where the index number is advanced by
one (see above), then we'll have a third routine to keep in sync.  Can
we use an automated theorem prover like Prover9 to ensure that this
code is right?


- stalemate pruning

This idea is to speed up processing by allowing promotion futurebases
with only queens to substitute for promotions into rooks and bishops,
since the only logical reason to underpromote is to avoid a stalemate.
To this end, a futurebase with an extra queen and stalemates pruned
would be treated as handling all three promotions (queen, rook, and
bishop).  To implement this correctly, you'd have to make sure that
stalemates were pruned not only in the current tablebase, but in all
(and I mean ALL) futurebases.  So this would mean adding an element to
tablebase-statistics, <all-stalemates-pruned color="white|black"/>,
that would flag when all futurebases depending on it had their
stalemates pruned as well.  This element would back propagate through
the futurebases and be dropped at any point where stalemate pruning
was turned off (unless there were no stalemates, like kk.htb).  In any
event, I've got the code to group all three promotions together in
RCS, but I've ripped it out of the main line since this extra logic
has never been implemented.


- ability to back prop from a Nalimov tablebase

Nalimov tablebases have been precomputed for 6-piece endgames;
currently, we've only got 5-piece endgames for Hoffman, so to leverage
the work that's already been done and to make Hoffman fully as
powerful as Freezer, we'd like to back prop directly from a Nalimov
tablebase.  With a good description of the Nalimov file format (which
I don't have), this shouldn't been difficult.  Also consider that
Hoffman tablebases are (currently) much bigger than Nalimov
tablebases, and fully 1TB of storage is required for a full set of
6-piece Nalimovs.


- more sophisticated futurebase handling

A current research target is to generate a full set of tablebases to
handle all king-and-pawn endgames, at least up to a queening solution.
We'd like to compute "white to queen and draw", which means that we
only treat a promotion move as a win if we've already got a forced
draw from that position.  So, we need to compute a drawing analysis,
then augment that into a queening analysis.


- restrict range of indices we look at during a backprop

If we're backproping, say, from kqk into kpk, we need only consider
those positions where the queen is on the eighth rank.  If the queen
is encoded into the MSBs of the index, then we should be able to
figure out at what futurebase index we need to start processing, and
fast forward to that point.  Symmetry might disrupt this, but the
general idea might still work, especially if it was done in a general
enough fashion, maybe by constructing a bitboard of legal board
positions for each piece at the beginning of a backprop (of any kind)
and fast forwarding over chunks of the futurebase that have been
identified as irrelevant.  The index-ordering=reverse attribute was
created to facilitate this, but it's never been implemented.


- add a repeat option for processing multiple XML files

If we're doing distributed batch processing, we'd probably like to
provide a URL to the program to get a generation control file (this
already works), then retry that URL when we're all done to get another
control file, and keep doing this.  Have to make sure the program is
pretty clean in its memory usage for this to work well.  Also should
provide some way to terminate the program at the end of the current
tablebase build.


- prune after some number of passes

If we're looking to trade some accuracy for speed, we can prune at
almost any point in the calculation by conceding every position that
hasn't been resolved.  So, for example, if we anticipate that allowing
black to queen will lose the game, except that we still want to
consider the possibility of immediately capturing the new queen
(Freezer can do this), we can run for one or two passes and then prune
everything else.  Note that in this example, we can already achieve
almost the same result by freezing the queen on the queening square
and conceding any move by the queen as a win to black.  We'd probably
run for several passes, though, checking lines where white can check
black around for a few moves before taking the queen.  Could also be
used to create faster test cases.


- some kind of GUI

I like leaving hoffman as a standalone computation engine.  For one
thing, this makes it easier to compile on a minimal system where
you're not going to do anything except farm out tablebase
calculations.  So my current thought for a GUI would be either a
Perl/Tk script or a Java applet, and I think Java is much cleaner than
Perl.  Freezer seems to have a nice GUI for a tablebase generator; see
the demos on http://www.freezerchess.com/

Even the computation engine could use a GUI, though, especially on
Microsoft systems where it's the standard user interface.  A modal
startup sequence could begin with radio buttons to select either a
file chooser to pick a control file (or files) for processing, or a
URL to connect to a remote webserver and retrieve control files from
there.  An options panel could allow selecting the maximum amount of
RAM to use, along with the location of a temporary files directory and
the maximum amount of disk space to use.  Use of proptable mode would
then be automatic depending on how much RAM is available.  It should
be possible to change these settings while the program is running.
Once the analysis proper starts, you should be able to halt or pause
it, as well as iconize it while it continues to run.


- pawngen

Needs to be more sophisticated.  Needs to handle restricted pieces.
Needs to apply pruning statements selectively in different situations
(there's already some code in it to do this using Perl expressions).

Check for conflicting prune statements, so we don't have to wait until
we get into a hoffman run before we find out about them.

Add ability to restrict piece locations.  This should let us do the
fortress example with a single control file.

Add 'freeze' pruning type for promotions - promote and freeze new
piece on the promotion square.  This lets us queen a pawn and then let
it sit there on the queening square to ensure that the other side
can't either capture it immediately or queen right after us.


- improve probe mode's move parser

Check illegal moves more aggressively; have a 'back' command.


- random access to tablebase files

Would require a modified gzip format.  Nalimov used a variant of
Lempel-Ziv developed by Andrew Kadatch; I'm not sure how it works.
Maybe preprocess the file to develop a single, constant Huffman table
and store it at the beginning of the file along with a table of byte
offsets that can be binary searched to figure where in the file you
need to start decompressing.  Other people have looked at this; a
literature search seems to be in order.


- eliminate extra proptable passes

We don't generate any output proptables when initializing the entries
table, then make our first intratable pass without any input
proptables.  Also, the final tablebase generation doesn't modify the
entries table at all, so there's no need to re-write it to disk, with
the corresponding slow encryption.


- revisit all the XXXs

- check stalemate handling in general
